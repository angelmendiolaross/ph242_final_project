---
title: "Introduction to Causal Inference - Final Project"
output: pdf_document
---

```{r setup, include=FALSE}
### Clear global environment
rm(list=ls())

# Install packages 
if (!require("pacman")) install.packages("pacman")

pacman::p_load(# Tidyverse packages including dplyr and ggplot2 
               tidyverse,
               ggthemes,
               tinytex, #latex
               magrittr, #%<>% operator
               here, 
               stargazer)

covid_df <- read_csv(here("data/county_level_covid19_dataset.csv"))

## Selecting only the variables we'll be using for analysis 
covid_df <- covid_df %>% dplyr::select(- `Predicted December Deaths`, - `Predicted December Cases`,        -`Predicted December Infection Rate (per 100,000)`, -`Predicted December Mortality Rate (per 100,000)`, -`Cumulative Mortality Rate (per 100,000)`, -`Cumulative Infection Rate (per 100,000)`)

covid_df <- covid_df %>% rename(
  mortality_jan = `January Mortality Rate (per 100,000)`,
  mortality_feb = `February Mortality Rate (per 100,000)`, 
  mortality_mar = `March Mortality Rate (per 100,000)`, 
  mortality_apr = `April Mortality Rate (per 100,000)`, 
  mortality_may = `May Mortality Rate (per 100,000)`, 
  mortality_jun = `June Mortality Rate (per 100,000)`, 
  mortality_jul = `July Mortality Rate (per 100,000)`, 
  mortality_aug = `August Mortality Rate (per 100,000)`, 
  mortality_sep = `September Mortality Rate (per 100,000)`, 
  mortality_oct = `October Mortality Rate (per 100,000)`, 
  mortality_nov = `November Mortality Rate (per 100,000)`, 
  mortality_dec = `December Mortality Rate (per 100,000)`, 
  infection_jan = `January Infection Rate (per 100,000)`, 
  infection_feb = `February Infection Rate (per 100,000)`, 
  infection_mar = `March Infection Rate (per 100,000)`, 
  infection_apr = `April Infection Rate (per 100,000)`, 
  infection_may = `May Infection Rate (per 100,000)`, 
  infection_jun = `June Infection Rate (per 100,000)`, 
  infection_jul = `July Infection Rate (per 100,000)`, 
  infection_aug = `August Infection Rate (per 100,000)`, 
  infection_sep = `September Infection Rate (per 100,000)`, 
  infection_oct = `October Infection Rate (per 100,000)`, 
  infection_nov = `November Infection Rate (per 100,000)`, 
  infection_dec = `December Infection Rate (per 100,000)`, 
  county_name = `County Name`, 
  state = State, 
  population = Population, 
  pct_unemployed = `% Unemployed`, 
  median_hh_income = `Median Household Income`, 
  population_density = `Approximate Population Density`, 
  african_american = `% African American`, 
  hispanic = `% Hispanic`,
  asian_american = `% Asian American`,
  white = `% White American`,
  native_alaskan = `% Native American and Alaska Native`,
  hawaiian_pacific = `% Native Hawaiian and Other Pacific Islander`,
  pct_college_associate = `Approx. % People with College or Associate's Degree per Year`, 
  mask_requirement = `Statewide Mask Requirement`)
```

## Preprocessing Data

```{r, echo=TRUE, results='hide'}
# transforming to a long dataset
infect <- covid_df %>%
  dplyr::select(countyFIPS, infection_jan, infection_feb, infection_mar, infection_apr, infection_may, 
         infection_jun, infection_jul, infection_aug,infection_sep, infection_oct, infection_nov, infection_dec)

infect_long <- gather(infect, month, infection_rate, c(infection_jan, infection_feb, infection_mar, infection_apr,
                                                infection_may, infection_jun, infection_jul, infection_aug,
                                                infection_sep, infection_oct, infection_nov, infection_dec
                                                ), factor_key=TRUE)
infect_long <- infect_long %>%
  arrange(countyFIPS)

# renaming months
infect_long %<>%
  separate(month, c(NA, "month"))

# joining with geographic and demographic data
wide_df <- covid_df[c(2:6,31:41)]
df <- left_join(x = infect_long,
                y = wide_df,
                by = "countyFIPS")

# rearranging our dataset
df <- df[c(1,4,5,6,2,3,7:18)]

# adding numerical month col
df %<>%
  mutate(mon = ifelse(month=="jan", 1, NA),
         mon = ifelse(month=="feb", 2, mon),
         mon = ifelse(month=="mar", 3, mon),
         mon = ifelse(month=="apr", 4, mon),
         mon = ifelse(month=="may", 5, mon),
         mon = ifelse(month=="jun", 6, mon),
         mon = ifelse(month=="jul", 7, mon),
         mon = ifelse(month=="aug", 8, mon),
         mon = ifelse(month=="sep", 9, mon),
         mon = ifelse(month=="oct", 10, mon),
         mon = ifelse(month=="nov", 11, mon),
         mon = ifelse(month=="dec", 12, mon))

```

```{r, echo=TRUE}
# looking at infection rates over time
ggplot(df, aes(x=month, y=infection_rate, group = countyFIPS)) + 
  geom_line() +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Infection Rates by County and Month in 2020') +
  xlab('Month') +
  scale_x_discrete(limits=c("jan","feb","mar","apr","may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")) +
  ylab('Infection Rate')
```
In order to assess the impact of statewide mask mandates over time, we need to refine the dataset such that our treatment indicator shows the *month* each state passed the mandate (rather than just whether or not they passed it). We created a supplemental dataset based on news reports which identifies (1) the month that each state passed statewide mandates or (2) the month that the first city/county passed a mask mandate for those states with mandates in some parts. 

```{r, echo=TRUE, results='hide'}
# load dataset of when states passed their statewide mandates
states <- read_csv("data/state_mandates_month.csv")
states %<>%
  mutate(treat_date = ifelse(date=="apr", 4, NA),
         treat_date = ifelse(date=="may", 5, treat_date),
         treat_date = ifelse(date=="jun", 6, treat_date),
         treat_date = ifelse(date=="jul", 7, treat_date),
         treat_date = ifelse(date=="aug", 8, treat_date),
         treat_date = ifelse(date=="nov", 11, treat_date))
states_month <- df %>%
  group_by(state) %>%
  dplyr::select(state, month)

states_month <- left_join(states_month,states,by="state")
# deleting duplicates
states_month %<>%
    distinct(state, month, .keep_all = TRUE)

# adding numerical month col
states_month %<>%
  mutate(mon = ifelse(month=="jan", 1, NA),
         mon = ifelse(month=="feb", 2, mon),
         mon = ifelse(month=="mar", 3, mon),
         mon = ifelse(month=="apr", 4, mon),
         mon = ifelse(month=="may", 5, mon),
         mon = ifelse(month=="jun", 6, mon),
         mon = ifelse(month=="jul", 7, mon),
         mon = ifelse(month=="aug", 8, mon),
         mon = ifelse(month=="sep", 9, mon),
         mon = ifelse(month=="oct", 10, mon),
         mon = ifelse(month=="nov", 11, mon),
         mon = ifelse(month=="dec", 12, mon))

# creating treatment variable
states_month %<>%
  group_by(state) %>%
  mutate(treatment = ifelse(mon>=treat_date,1,0)) 

# assign SD as nontreated (the only state that never passed a mask mandate)
states_month %<>%
  mutate(treatment = ifelse(state=="SD",0,treatment))

# cleaning up dataframe
state_df <- states_month[c(1,2,6,8)]

# join with main df
df <- left_join(x = df,
                  y = state_df,
                  by = c("state", "month"))

# load kansas county dataset
kansas <- read_csv("data/kansas_counties.csv")

# getting county fips that passed mandate
kansas_nomask <- kansas %>% subset(is.na(mask_mandate)) %>%
  dplyr::select(countyFIPS)
  
kansas_nomask <- dplyr::pull(kansas_nomask, countyFIPS)

# updating kansas counties that did not pass mask mandates
df %<>%
  mutate(treatment = ifelse(countyFIPS %in% kansas_nomask, 0, treatment))

# updating Tennessee counties that never had mask mandate
tn_nomask <- c("47021", "47043", "47101", "47081")
df %<>%
  mutate(treatment = ifelse(countyFIPS %in% tn_nomask, 0, treatment))

```
Kansas is a unique case with mask mandate data available by county. On July 2, 2020, the governor of Kansas issued a state mandate, effective July 3, requiring masks or other face coverings in public spaces. As of August 11, 24 of Kansasâ€™s 105 counties did not opt out of the state mandate. 81 counties opted out of the state mandate, as permitted by state law, and did not adopt their own mask mandate. These 81 counties are regarded as untreated in our dataset. Source: <https://www.cdc.gov/mmwr/volumes/69/wr/mm6947e2.htm>

Four Tennessee counties that have never had a mask mandate: Cheatham, Dickson, Lewis and Hickman. These are regarded as untreated in the dataset. Source: <https://fox17.com/news/local/here-are-the-tennessee-counties-where-masks-are-mandated-right-now-thanksgiving-covid-19-travel-nashville-williamson-davidson-wilson-rutherford-sumner-montgomery-henry-robertson-wayne-warren>

Mississippi was the first state to lift state-wide mask mandate in October 2020. <https://www.forbes.com/sites/nicholasreimann/2020/09/30/mississippi-becomes-first-state-to-lift-mask-mandate/?sh=16d8c2667f13>

Finally, because unemployment changed so radically in 2020 and may be an important confounder, we bring in monthly unemployment data from the BLS. <https://www.bls.gov/web/metro/laucntycur14.txt>

```{r, echo=TRUE, results='hide'}
state_unemp <- read_csv("data/county_employment_rates.csv")
# deleting preliminary data
state_unemp %<>%
  subset(Period!="Feb-21(p)")
state_unemp_sm <- state_unemp[c(4,5,6,8,9)]

state_unemp_sm %<>%
  separate(Period, c("month", "year"))
state_unemp_sm$month <- tolower(state_unemp_sm$month)

state_unemp_sm %<>% rename(county_name = `Area Title`)
# joining

df <- left_join(x = df,
                  y = state_unemp_sm,
                  by = c("county_name", "month"))

# deleting duplicates
df %<>%
    distinct(countyFIPS, month, .keep_all = TRUE)

# rearranging and saving final df file
df <- df[c(1:5,19,21,18,6,7:17,23,24,25,20)]

# assessing missingness overall
sum(is.na(df)) # 792

# problem is SD treat_date is NA - fixing this
df %<>%
  mutate(treat_date=ifelse(state=="SD",0,treat_date))
```

## Specify the Scientific Question
What is the effect of state mandates to wear a mask on Covid-19 infection rates in US counties during 2020? 

This is a causal question - and not a statistical question - because we are not just interested in the observed association between mask enforcement by states and Covid-19 infection rate. We want to know what infection rates would have been if mask enforcement by states would have been generated differently, for example, by mandating all adults to wear a mask, or mandating no adults to wear a mask. The target population are US counties from January to December 2020. 

## Specify a Casual Model

**Endogenous variables:** 
Endogenous variables are factors that inform the scientific question we're asking, and of which we have a certain level of knowledge. Here, we're including all observable factors that are present in our data, but we could potentially include some unobservables as well, which we will address later. 
$Y$ = Infection rates at the county-level (for each month from Jan to Dec 2020)
$A$ = The exposure (state-wide mask mandate) has three possible levels: 
$\mathcal{A} = {0, 1, 2}$
These levels correspond to mask enforcement at the state-level (0 = No, 1 = Some parts, 2 = Yes).

$W$ represents the following covariates:
$W_1$ = Median income at the county level
$W_2$ = % of Whites at the county level
$W_3$ = % of Hispanic at the county level
$W_4$ = % of African Americans at the county level
$W_5$ = % of Asian Americans at the county level
$W_6$ = % of residents who finished college or associate education at the county level
$W_7$ = % unemployed (by month) at the county level

Given our outcome of interest, treatment and potential confounders, we cannot make any independence assumptions or exclusion restrictions. The economic structure or policies developed at the county level can all be shared common causes of median income, race, educational attainment and employment. 

**Exogenous variables: Unmeasured confounders:** 
The exogenous variables include all the unmeasured factors that determine the values that the endogenous variables take. $U$ is a placeholder for all the shared causes between our endogenous variables that we do not know (it's not the random input to the system, just the __shared__ random input). 

In this study, the exogenous nodes are $U=(U_{W1},U_{W2},U_{W3},U_{W4},U_{W5}, U_{W6}, U_{W7},U_A,U_Y) \sim P_U$:

$U_{W1}$ to $U_{W4}$ refers to the unmeasured confounders related to the percentage of White, African American, Asian and Hispanic residents at the county level. These could be: economic structure, history of segregation, political party in the majority. 

$U_{W5}$ refers to the unmeasured confounders related to median income at the county level. These could be: economic structure, minumum wage, welfare policies. 

$U_{W6}$ refers to the unmeasured confounders related to the percentage of residents who finished college or associate education at the county level. These could be: economic structure, welfare policies, quality of public school system, nurturing culture within the household.

$U_{W7}$ refers to the unmeasured confounders related to the unemployment rate at the county level. These could be: economic structure, minimum wage, quality of unemployment services. 

$U_A$ refers to the unmeasured confounders related to mask enforcement by the state. These could be: political pressure from different constituencies to either pass or abstain from passing a mask mandate.

$U_Y$ refers to the unmeasured confounders related to mask enforcement by the state. These could be: genetic factors, medications, health care access, and travel behavior.

**Structural Causal Model**
$W_1 = f_{W1}(U_{W1})$
$W_2 = f_{W2}(W_1,U_{W2})$
$W_Î© = f_{W3}(W_1,W_2,U_{W3})$
$W_4 = f_{W4}(W_1,W_2,U_{W3}, U_{W4})$
$W_5 = f_{W5}(W_1,W_2,U_{W3}, U_{W4}, U_{W5})$
$W_6 = f_{W6}(W_1,W_2,U_{W3}, U_{W4}, U_{W5}, U_{W6})$
$W_7 = f_{W6}(W_1,W_2,U_{W3}, U_{W4}, U_{W5}, U_{W6}, U_{W7})$
$A = f_A(W_1,W_2,W_3,W_4,W_5,W_6,W_7,U_A)$
$Y = f_Y(W_1,W_2,W_3,W_4,W_5,W_6,W_7,A,U_Y)$
**Comment BS: Usually there's a time ordering to think about the W's. I don't know how much that exactly applies here, but does it make sense to have race, then median income, then unemployment?**  
```{r}
suppressPackageStartupMessages(library(tidyverse))
library(ggdag)

coords <- list(
    x = c(A = 0, W1 = 1, W2 = 5, W3 = 9, W4 = 13, W5 = 17, W6 = 21, U = 10, Y = 23),
    y = c(A = 2, W1 = 10, W2 = 10, W3 = 10, W4 = 10, W5 = 10, W6 = 10, U = -5, Y = 2))
    
dagify(Y ~ W1 + W2 + W3 + W4 + W5 + W6 + W7 + A + U,  
       A ~ W1 + W2 + W3 + W4 + W5 + W6 + W7 + U, 
       W1 ~ U, 
       W2 ~ W1 + U, 
       W3 ~ W1 + W2 + U, 
       W4 ~ W1 + W2 + W3 + U, 
       W5 ~ W1 + W2 + W3 + W4 + U,
       W6 ~ W1 + W2 + W3 + W4 + W5 + U,
       W7 ~ W1 + W2 + W3 + W4 + W5 + W6 + U,
       coords=coords) %>% 
       ggdag() + 
       theme_dag()
```

**Comment BS: This is the DAG with no exclusion restrictions or independence assumptions. Does it make sense? We can make it outside of R, but I don't know how to make it better with this code. If anybody knows how to, it'd be great if the arrows coming from U were dashed. Also, I see a case for reverse causality here. We can draw it and then assume it away in the d-separation convenience assumptions, or we can say that for the purpose of this study we don't consider it a possibility from the get go.** 

## Translate your question into a formal target casual parameter defined using counterfactuals
The intervention of interest is forcing each individual to have a range of mask enforcement $a \in \mathcal{A}$. We are interested in exploring three levels in our intervention of interest: 
0 = No mask enforcement at the county level, 
1= Mask enforcement in some parts of the county, 
2 = Mask enforcement in the whole county.

The counterfactuals of interest are ($Y_a:a \in \mathcal{A}$), where A are the set of mask enforcement levels of interest. The counterfactual $Y_a$ is the infection rate if, possibly contrary to fact, the individual lived in a state with mask enforcement A=a from the January to December 2020 period.

Counterfactuals of interest: 
$Y_a = {f}_Y (W_1, W_2, W_3, W_4, W_5, W_6, W_7, a, U_Y) (a \in \mathcal{A})$ = {0, 1, 2}
where $\mathcal{A}$ refers to treatment levels of interest

A marginal structural model provides a summary measure of how the expectation of the counterfactual outcome changes as a function of treatment. Given that our treatment has three potential levels of exposure, we think a marginal structural model is the most suitable way to represent this relationship. 

$\mathbb{E}_{U,X}(Y_a)$ = $m(a|\beta)$
$\beta(P_{U,X|m})\equiv arg min_\beta \mathbb{E}_{U,X} [\sum_{a \in \mathcal{A}}(Y_a = m(a|\beta)^2)]$

**In the lecture notes it says that for MSM intervention variable is continuous or ordinal with a lot of levels. Maybe ask Shalika/Jean whether we're fine here using an MSM with only three levels?**

## Specify your observed data and its link to the casual model
*Describe the data you are working with and its link to the casual model you have specified.*

We assume the observed data were generated by sampling n times from a data generating system contained in the structural causal model $\mathcal{M}\mathcal{F}$, resulting in n.i.i.d copies of $O_1$, $O_2$, $O_3$,...$O_n$ drawn from probability distribution $P_0$. This provides a link between the causal model $\mathcal{M}\mathcal{F}$ and the statistical model $\mathcal{M}$.

The statistical model $\mathcal{M}$ is the set of possible observed data distributions. We have not placed any restrictions on the statistical model, which is thereby non-parametric.

*Be  sure  to  include  a  basic  descriptive  table  of  your  data  that  provides information on the outcome, exposure, and covariate distributions.*
```{r, echo=F, results='asis'}
# reducing just to main variables
stats <- df[c(9:10,12:20,23)]
stargazer(as.data.frame(stats),
          title = "Descriptive Statistics",
          digits=2,
          header = FALSE,
          covariate.labels = c("Infection Rate",
                               "Population", 
                               "Median Household Income", 
                               "Population Density", 
                               "Percent African American", 
                               "Percent Latino", "Percent Asian American", 
                               "Percent White", 
                               "Percent Native American", 
                               "Percent Pacific Islander", 
                               "Percent College", 
                               "Unemployment Rate"))
`````

## Identify
**Is your target casual parameter identified under your initial causal model?**
Given that we're not making any assumptions on independence or exclusion restrictions, we won't be able to identify the causal parameter under our initial causal model because our covariates and the unmeasured confounders have open paths to A. 

**If not, under what additional assumptions would it be identified?**
We would need to make some convenience assumptions: 

1. We would need to condition on all covariates (median income, race, education and unemployment). Doing this would block all paths from these variables to A and would not create any new paths. 

However, assumption 1 is not enough because there's a remaining backdoor path to A still open via the $U_Y$ and $U_A$ that connects Y and A, and opens a backdoor back to A. So we add assumption 2:

2. We would need to assume that there are no unmeasured shared causes between A and Y. For identifiability to hold, we must be sure that all of the observed association between A and Y is due to the causal effect we're interested in. Therefore, we need to additionally assume that Y does not affect A (i.e. that there's no reverse causality) for the purpose of this project, though in the real-world it'd be sensible to argue that an increase in the infection rate in a given state might affect the likelihood of the state deciding to implement a mask enforcement policy. 

We use $\mathcal{M}\mathcal{F}*$ to denote the original SCM, augmented with additional assumptions needed for identifiability.  

**How plausible  are  these  for  your  particular  problem?**
**Comment BS: Any thoughts?**
We think these assumptions are too strong and might not hold in a real-life scenario. 

**Are there additional data/changes to your study design that would improve their plausibility?**
**Comment BS: Let's consider other potentially measured variables we're not including in W because we don't have them in the dataset that might be relevant confounders. How important are these for the model we're proposing?**
**Comment BS: Any thoughts?** 

## Commit to a Statistical Model and Estimand (Target parameter of the observed data distribution).
The SCM, which is a model on $P_{U,X}$ implies a model $\mathcal{M}$ on $P_0$. This is our statistical model. A statistical model is a set of allowed distributions for the observed data. 

In this case, given the data we have and the convenience assumptions made, we will be working with a semi-parametric model (?). **I'm not too sure about how to think about this**

The target parameter of the observed data distribution (the __estimand__) given our convenience assumptions, would be the following: 
$[\mathbb{E}(Y|A=1,{W_1,W_2,W_3,W_4,W_5,W_6,W_7}) - \mathbb{E}(Y|A=0,W_1,W_2,W_3,W_4,W_5,W_6,W_7)]$ **I realized I don't know how to write this for a MSM, but I'll update when I find this information on the class lectures**

## Estimate.
**Apply each of the three estimators we have learned in class (simple or non-targeted substitution estimator-a.k.a G-computation estimator), Inverse probability of treatment weighted estimator, and TMLE) to estimate your target parameter. Use of the tmle, ltmle, or other Rpackages is acceptable. Also report unadjusted results for comparison.**

## G-computation estimator

**Can we abbreviate this to W? Also, I don't know how to make the notation for the 7 W's in the marginal distribution?**
$\mathbb{E}_{U,X} = \sum_{w} \mathbb{E}_0[\mathbb{E}(Y|A=a,W_1,W_2,W_3,W_4,W_5,W_6,W_7)P_0(W=w)$ 

## Inverse probabiluty if treatment weighted estimator

## TMLE 

**Use Super Learner when implementing TMLE. For comparison, you may wish to use it when implementing your G-computation and IPTW estimators also. A simple library is fine (writing wrappers to include your own parametric regressions as candidates is great). Include an assessment of the performance (cross validated risk) of the algorithms in your library.It is helpful to include the simple mean as a benchmark.  Also report an estimate of the cross-validated risk of the SL and interpret.**

**Provide some formally assessment of the positivity assumption. Evaluate the distribution of your estimated propensity score gn(A= 1|Wi), i=1, ..., n, and corresponding non-stabilized weights (as well as of your stabilized weights if you use stabilized weights to fit an MSM). Consider evaluating sensitivity to different truncation levels for gn. Note that for TMLE, bounding or truncating gn away from 0 is recommended on the basis of both theory and finite sample performance; for IPTW it can help or hurt.  Report how for how many observations was gn(A|Wi) truncated.**

**Present a detailed plan for statistical inference/variance estimation based on the non-parametric bootstrap, and implement it (understanding that time  may be  a limitation  depending on  your  SL  library). When bootstrapping an estimator that uses cross-validation to estimate nuisance parameters, be careful to ensure that all copies of a given independent unit are contained within the same fold. Plot your bootstrap distribution and comment as appropriate. For TMLE (and IPTW), you can also report a influence curve based variance estimate for comparison, if you wish.**

## Interpret results.
**What is the statistical interpretation of your analyses? Discuss differences (or lack thereof) in the estimates provided by the different estimators. What is the causal interpretation of your results and how plausible is it?  What are key limitations of your analysis? How might these results (if at all) inform policy, understanding, and/or the design of future studies?**
