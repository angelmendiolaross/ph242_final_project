---
title: "Introduction to Causal Inference - Final Project"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
### Clear global environment
rm(list=ls())

# Install packages 
if (!require("pacman")) install.packages("pacman")

pacman::p_load(# Tidyverse packages including dplyr and ggplot2 
               tidyverse,
               ggthemes,
               tinytex, #latex
               magrittr, #%<>% operator
               here, 
               stargazer, 
               corrplot,
               ggdag, 
               SuperLearner,
               polspline, 
               nnet, 
               ltmle, 
               MLmetrics, 
               MASS, 
               sqldf)

covid_df <- read_csv(here("data/county_level_covid19_dataset.csv"))

## Selecting only the variables we'll be using for analysis 
covid_df <- covid_df %>% dplyr::select(- `Predicted December Deaths`, - `Predicted December Cases`,        -`Predicted December Infection Rate (per 100,000)`, -`Predicted December Mortality Rate (per 100,000)`, -`Cumulative Mortality Rate (per 100,000)`, -`Cumulative Infection Rate (per 100,000)`)

covid_df <- covid_df %>% rename(
  mortality_jan = `January Mortality Rate (per 100,000)`,
  mortality_feb = `February Mortality Rate (per 100,000)`, 
  mortality_mar = `March Mortality Rate (per 100,000)`, 
  mortality_apr = `April Mortality Rate (per 100,000)`, 
  mortality_may = `May Mortality Rate (per 100,000)`, 
  mortality_jun = `June Mortality Rate (per 100,000)`, 
  mortality_jul = `July Mortality Rate (per 100,000)`, 
  mortality_aug = `August Mortality Rate (per 100,000)`, 
  mortality_sep = `September Mortality Rate (per 100,000)`, 
  mortality_oct = `October Mortality Rate (per 100,000)`, 
  mortality_nov = `November Mortality Rate (per 100,000)`, 
  mortality_dec = `December Mortality Rate (per 100,000)`, 
  infection_jan = `January Infection Rate (per 100,000)`, 
  infection_feb = `February Infection Rate (per 100,000)`, 
  infection_mar = `March Infection Rate (per 100,000)`, 
  infection_apr = `April Infection Rate (per 100,000)`, 
  infection_may = `May Infection Rate (per 100,000)`, 
  infection_jun = `June Infection Rate (per 100,000)`, 
  infection_jul = `July Infection Rate (per 100,000)`, 
  infection_aug = `August Infection Rate (per 100,000)`, 
  infection_sep = `September Infection Rate (per 100,000)`, 
  infection_oct = `October Infection Rate (per 100,000)`, 
  infection_nov = `November Infection Rate (per 100,000)`, 
  infection_dec = `December Infection Rate (per 100,000)`, 
  county_name = `County Name`, 
  state = State, 
  population = Population, 
  pct_unemployed = `% Unemployed`, 
  median_hh_income = `Median Household Income`, 
  population_density = `Approximate Population Density`, 
  african_american = `% African American`, 
  hispanic = `% Hispanic`,
  asian_american = `% Asian American`,
  white = `% White American`,
  native_alaskan = `% Native American and Alaska Native`,
  hawaiian_pacific = `% Native Hawaiian and Other Pacific Islander`,
  pct_college_associate = `Approx. % People with College or Associate's Degree per Year`, 
  mask_requirement = `Statewide Mask Requirement`)
```

## Preprocessing Data

Preprocessing included first transforming our dataset from wide to long.

```{r, include=FALSE}
# transforming our to a long dataset
infect <- covid_df %>%
  dplyr::select(countyFIPS, infection_jan, infection_feb, infection_mar, infection_apr, infection_may, 
         infection_jun, infection_jul, infection_aug,infection_sep, infection_oct, infection_nov, infection_dec)

infect_long <- gather(infect, month, infection_rate, c(infection_jan, infection_feb, infection_mar, infection_apr,
                                                infection_may, infection_jun, infection_jul, infection_aug,
                                                infection_sep, infection_oct, infection_nov, infection_dec
                                                ), factor_key=TRUE)
infect_long <- infect_long %>%
  arrange(countyFIPS)

# renaming months
infect_long %<>%
  separate(month, c(NA, "month"))

# joining with geographic and demographic data
wide_df <- covid_df[c(2:6,31:41)]
df <- left_join(x = infect_long,
                y = wide_df,
                by = "countyFIPS")

# rearranging our dataset
df <- df[c(1,4,5,6,2,3,7:18)]

# adding numerical month col
df %<>%
  mutate(mon = ifelse(month=="jan", 1, NA),
         mon = ifelse(month=="feb", 2, mon),
         mon = ifelse(month=="mar", 3, mon),
         mon = ifelse(month=="apr", 4, mon),
         mon = ifelse(month=="may", 5, mon),
         mon = ifelse(month=="jun", 6, mon),
         mon = ifelse(month=="jul", 7, mon),
         mon = ifelse(month=="aug", 8, mon),
         mon = ifelse(month=="sep", 9, mon),
         mon = ifelse(month=="oct", 10, mon),
         mon = ifelse(month=="nov", 11, mon),
         mon = ifelse(month=="dec", 12, mon))

```

```{r, echo=TRUE}
# looking at infection rates over time
ggplot(df, aes(x=month, y=infection_rate, group = countyFIPS)) + 
  geom_line() +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Infection Rates by County and Month in 2020') +
  xlab('Month') +
  scale_x_discrete(limits=c("jan","feb","mar","apr","may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")) +
  ylab('Infection Rate')
```
In order to assess the impact of statewide mask mandates over time, we need to refine the dataset such that our treatment indicator shows the *month* each state passed the mandate (rather than just whether or not they passed it). We created a supplemental dataset based on news reports which identifies (1) the month that each state passed statewide mandates or (2) the month that the first city/county passed a mask mandate for those states with mandates in some parts. 

```{r, include=FALSE}
# load dataset of when states passed their statewide mandates
states <- read_csv("data/state_mandates_month.csv")
states %<>%
  mutate(treat_date = ifelse(date=="apr", 4, NA),
         treat_date = ifelse(date=="may", 5, treat_date),
         treat_date = ifelse(date=="jun", 6, treat_date),
         treat_date = ifelse(date=="jul", 7, treat_date),
         treat_date = ifelse(date=="aug", 8, treat_date),
         treat_date = ifelse(date=="nov", 11, treat_date))
states_month <- df %>%
  group_by(state) %>%
  dplyr::select(state, month)

states_month <- left_join(states_month,states,by="state")
# deleting duplicates
states_month %<>%
    distinct(state, month, .keep_all = TRUE)

# adding numerical month col
states_month %<>%
  mutate(mon = ifelse(month=="jan", 1, NA),
         mon = ifelse(month=="feb", 2, mon),
         mon = ifelse(month=="mar", 3, mon),
         mon = ifelse(month=="apr", 4, mon),
         mon = ifelse(month=="may", 5, mon),
         mon = ifelse(month=="jun", 6, mon),
         mon = ifelse(month=="jul", 7, mon),
         mon = ifelse(month=="aug", 8, mon),
         mon = ifelse(month=="sep", 9, mon),
         mon = ifelse(month=="oct", 10, mon),
         mon = ifelse(month=="nov", 11, mon),
         mon = ifelse(month=="dec", 12, mon))

# creating treatment variable
states_month %<>%
  group_by(state) %>%
  mutate(treatment = ifelse(mon>=treat_date,1,0)) 

# assign SD as nontreated (the only state that never passed a mask mandate)
states_month %<>%
  mutate(treatment = ifelse(state=="SD",0,treatment))

# cleaning up dataframe
state_df <- states_month[c(1,2,6,8)]

# join with main df
df <- left_join(x = df,
                  y = state_df,
                  by = c("state", "month"))

# load kansas county dataset
kansas <- read_csv("data/kansas_counties.csv")

# getting county fips that passed mandate
kansas_nomask <- kansas %>% subset(is.na(mask_mandate)) %>%
  dplyr::select(countyFIPS)
  
kansas_nomask <- dplyr::pull(kansas_nomask, countyFIPS)

# updating kansas counties that did not pass mask mandates
df %<>%
  mutate(treatment = ifelse(countyFIPS %in% kansas_nomask, 0, treatment))

# updating Tennessee counties that never had mask mandate
tn_nomask <- c("47021", "47043", "47101", "47081")
df %<>%
  mutate(treatment = ifelse(countyFIPS %in% tn_nomask, 0, treatment))
```

Kansas is a unique case with mask mandate data available by county. On July 2, 2020, the governor of Kansas issued a state mandate, effective July 3, requiring masks or other face coverings in public spaces. As of August 11, 24 of Kansasâ€™s 105 counties did not opt out of the state mandate. 81 counties opted out of the state mandate, as permitted by state law, and did not adopt their own mask mandate. These 81 counties are regarded as untreated in our dataset. Source: <https://www.cdc.gov/mmwr/volumes/69/wr/mm6947e2.htm>

Four Tennessee counties that have never had a mask mandate: Cheatham, Dickson, Lewis and Hickman. These are regarded as untreated in the dataset. Source: <https://fox17.com/news/local/here-are-the-tennessee-counties-where-masks-are-mandated-right-now-thanksgiving-covid-19-travel-nashville-williamson-davidson-wilson-rutherford-sumner-montgomery-henry-robertson-wayne-warren>

Finally, because unemployment changed so radically in 2020 and may be an important confounder, we also bring in monthly unemployment data from the BLS. <https://www.bls.gov/web/metro/laucntycur14.txt>

```{r, include=FALSE}
state_unemp <- read_csv("data/county_employment_rates.csv")
# deleting preliminary data
state_unemp %<>%
  subset(Period!="Feb-21(p)")
state_unemp_sm <- state_unemp[c(4,5,6,8,9)]

state_unemp_sm %<>%
  separate(Period, c("month", "year"))
state_unemp_sm$month <- tolower(state_unemp_sm$month)

state_unemp_sm %<>% rename(county_name = `Area Title`)
# joining

df <- left_join(x = df,
                  y = state_unemp_sm,
                  by = c("county_name", "month"))

# deleting duplicates
df %<>%
    distinct(countyFIPS, month, .keep_all = TRUE)

# rearranging and saving final df file
df <- df[c(1:5,19,21,18,6,7:17,23,24,25,20)]

# assessing missingness overall
sum(is.na(df)) # 792

# problem is SD treat_date is NA - fixing this
df %<>%
  mutate(treat_date=ifelse(state=="SD",0,treat_date))
```

```{r}
## Checking correlations between individual variables within our W to make sure that there's not gonna be any multi-collinearity issues! ##
just_ws=covid_df %>% ## Using covid_df because everything we need is here and not inflating size of sample by time unnecessarily - doesn't change correlation, but changes significance, if we want to look at that## 
  dplyr::select(pct_unemployed, median_hh_income, population, population_density, african_american, hispanic, asian_american, white, native_alaskan, hawaiian_pacific, pct_college_associate)

cor_justws=cor(just_ws)

corrplot(cor_justws, type="upper", order="hclust", sig.level = 0.01, insig = "blank")
```
We seem to have a multi-collinearity issue: the percentage of the population identifying as White, Non-Hispanic/Latinx is highly negatively correlated with the percentage of the population identifying as Black. 

## Specify the Scientific Question
What is the effect of state mandates to wear a mask on Covid-19 infection rates in US counties during 2020? 
This is a causal question - and not a statistical question - because we are not just interested in the observed association between mask enforcement by states and Covid-19 infection rate. We want to know what infection rates would have been if mask enforcement by states would have been generated differently, for example, by mandating all adults to wear a mask, or mandating no adults to wear a mask. The target population are US counties from January to December 2020. 

## Specify a Causal Model

**Endogenous variables:** 
Endogenous variables are factors that inform the scientific question we're asking, and of which we have a certain level of knowledge. Here, we're including all observable factors that are present in our data, but we could potentially include some unobservables as well, which we will address later. 
$infection_rate$ = Infection rates at the county-level (for each month from Jan to Dec 2020)
$A$ = The exposure (state-wide mask mandate) has three possible levels: 
$\mathcal{A} = \{0, 1, 2\}$. 
These levels correspond to mask enforcement at the state-level (0 = No state-level mask enforcement, 1 = Mask enforcement only in some parts of the state, 2 = Yes, full, state-level mask enforcement).

$W$ represents the following covariates:
\begin{itemize}
  \item Median income at the county level
  \item % of Whites at the county level
  \item % of Hispanic at the county level
  \item % of African Americans at the county level
  \item % of Asian Americans at the county level
  \item % of residents who finished college or associate education at the county level
  \item % unemployed (by month) at the county level
  \item population of the county
  \item population density of the county
\end{itemize}

Given our outcome of interest, treatment and potential confounders, we cannot make any independence assumptions or exclusion restrictions. The economic structure or policies developed at the county level can all be shared common causes of median income, race, educational attainment and employment. 

**Exogenous variables: Unmeasured confounders:** 
The exogenous variables include all the unmeasured factors that determine the values that the endogenous variables take. $U$ is a placeholder for all the shared causes between our endogenous variables that we do not know (it's not the random input to the system, just the __shared__ random input). 

In this study, the exogenous nodes are $U=(U_{W1},U_{W2},U_{W3},U_{W4},U_{W5}, U_{W6}, U_{W7},U_A,U_Y) \sim P_U$:

$U_Y$ refers to the unmeasured confounders of the set of covariates: 

The unmeasured confounders related to the percentage of White, African American, Asian and Hispanic residents at the county level could be: economic structure, history of segregation, and political party in the majority. 

The unmeasured confounders related to median income at the county level could be: economic structure, minumum wage, welfare policies. 

The unmeasured confounders related to the percentage of residents who finished college or associate education at the county level could be: economic structure, welfare policies, quality of public school system, nurturing culture within the household.

The unmeasured confounders related to the unemployment rate at the county level could be: economic structure, minimum wage, quality of unemployment services. 

The unmeasured confounders related to the population and population density at the county level could be: political preference and/or partisanship of the county, presence of undocumented individuals. 

The unmeasured confounders related to mask enforcement by the state could be: political pressure from different constituencies to either pass or abstain from passing a mask mandate.

$U_Y$ refers to the unmeasured confounders related to mask enforcement by the state. These could be: genetic factors, medications, health care access, and travel behavior.

**Structural Causal Model**
$W = f_{W}(U_{W})$
$A = f_A(W, U_A)$
$infection_rate = f_Y(W,A,U_Y)$

```{r}
coords <- list(
    x = c(A = 0, W = 5, U = 5, Y = 10),
    y = c(A = 2, W = 10, U = 15, Y = 2))
    
dagify(Y ~ W + A + U,  
       A ~ W + U, 
       W ~ U, 
       coords=coords) %>% 
       ggdag() + 
       theme_dag()
```

```{r}
## Checking to make sure that the treatment variable (A) is county- or state-specific (seems to be state specific, so we'll be sure to implement clustering like Maya, Jean, and Shalika talked about) ##

## This will also be a limitation. ##
covid_df %>%
  dplyr::select(county_name, state, mask_requirement) %>%
  filter(state=="FL") 
```

## Translate your question into a formal target casual parameter defined using counterfactuals
The intervention of interest is forcing each state to have a range of mask enforcement $a \in \mathcal{A}$. We are interested in exploring three levels in our intervention of interest: 
0 = No mask enforcement at the state level, 
1 = Mask enforcement in some parts of the state, 
2 = Mask enforcement in the whole state.

The counterfactuals of interest are ($\text{infection_rate}_a:a \in \mathcal{A}$), where A are the set of mask enforcement levels of interest. The counterfactual $\text{infection_rate}_a$ is the infection rate if, possibly contrary to fact, the state had a mask enforcement level $A=a$ from the January to December 2020 period.

Counterfactuals of interest: 
$\text{infection_rate}_a = {f}_Y (W, a, U_Y) (a \in \mathcal{A})$ = {0, 1, 2}
where $\mathcal{A}$ refers to treatment levels of interest

A marginal structural model provides a summary measure of how the expectation of the counterfactual outcome changes as a function of treatment. Given that our treatment has three potential levels of exposure, we think a marginal structural model is the most suitable way to represent this relationship. 

$\mathbb{E}_{U,X}(Y_a) = m(a|\beta)$
$\beta(P_{U,X|m})\equiv arg min_\beta \mathbb{E}_{U,X} [\sum_{a \in \mathcal{A}}(Y_a = m(a|\beta)^2)]$

## Specify your observed data and its link to the casual model
*Describe the data you are working with and its link to the casual model you have specified.*

We assume the observed data were generated by sampling $n$ times from a data generating system contained in the structural causal model $\mathcal{M}\mathcal{F}$, resulting in $n$ i.i.d copies of $O_1$, $O_2$, $O_3$,...$O_n$ drawn from probability distribution $P_0$. This provides a link between the causal model $\mathcal{M}^\mathcal{F}$ and the statistical model $\mathcal{M}$.

The statistical model $\mathcal{M}$ is the set of possible observed data distributions. We have not placed any restrictions on the statistical model, which is thereby non-parametric.

## Identify
**Is your target casual parameter identified under your initial causal model?**
Given that we're not making any assumptions on independence or exclusion restrictions, we won't be able to identify the causal parameter under our initial causal model because our covariates and the unmeasured confounders have open paths to $A$. 

**If not, under what additional assumptions would it be identified?**
We would need to make some convenience assumptions: 

1. We would need to condition on all covariates (median income, race, education and unemployment). Doing this would block all paths from these variables to A and would not create any new paths. 

However, assumption 1 is not enough because there's a remaining backdoor path to A still open via the $U_Y$ and $U_A$ that connects Y and A, and opens a backdoor back to A. So we add assumption 2:

2. We would need to assume that there are no unmeasured shared causes between A and Y. For identifiability to hold, we must be sure that all of the observed association between A and Y is due to the causal effect we're interested in. Therefore, we need to additionally assume that Y does not affect A (i.e. that there's no reverse causality) for the purpose of this project, though in the real-world it'd be sensible to argue that an increase in the infection rate in a given state might affect the likelihood of the state deciding to implement a mask enforcement policy. Formally, we need to make assumptions about the independence of the background factors: $U_A \perp U_Y$ and (a) $U_A \perp U_W$ or (b) $U_Y \perp U_W$.

We use $\mathcal{M}^\mathcal{F*}$ to denote the original SCM, augmented with additional assumptions needed for identifiability.

To identify $\mathbb{E}_{U,X}(Y_a)$ with the G-computation formula, we also need the positivity assumption to hold:
$$
\min_{a \in A} \mathbb{P}_0(A=a | W=w) >0
$$
for all $w$ for which $\mathbb{P}_0(W=w)>0$. In this case, there must be a positive probability of passing a mask mandate and not passing a mask mandate within strata of baseline covariates.

**How plausible  are  these  for  your  particular  problem?**
We think these assumptions are too strong and might not hold in a real-life scenario. Based on empirical literature, we know that COVID-19 spread is related to the following (Roy & Ghosh, 2020}):

\begin{itemize}
  \item Population density
  \item The amount of testing 
  \item Airport traffic
  \item Proportion of the population in a higher age groups (40 and above, and most especially 60+).
\end{itemize}

(Hyperlink latex thing was acting feisty, so just pasting the link here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241165#:~:text=Population%20density%2C%20testing%20numbers%20and,population%20density%20and%20airport%20traffic.)

***Specifically, we do not have data on the latter three*** and the amount of testing as well as the proportion of the population in a higher age group are likely to be contributing factors not only to infection rates, but also whether a state enforces mask-wearing. These data are certainly available from other sources, but we did not have time to acquire and merge the various data sources needed to accomplish this; however, this would certainly help us be more confident in making our necessary assumptions for model identification.

Given that most of our covariates are continuous, we will also likely run into practical positivity violations, especially considering that only one state (with 66 counties) did not pass a mask mandate in at least some parts of the state in 2020.

**Are there additional data/changes to your study design that would improve their plausibility?**
As detailed above, there are certainly additional data sources (e.g., CPS, as well as other COVID-related data sets) that we could incorporate to improve the plausibility of our assumptions! 

## Commit to a Statistical Model and Estimand (Target parameter of the observed data distribution).
The SCM, which is a model on $P_{U,X}$ implies a model $\mathcal{M}$ on $P_0$. This is our statistical model. A statistical model is a set of allowed distributions for the observed data. 

In this case, given the data we have and the convenience assumptions made, we will be working with a semi-parametric model (?). **I'm not too sure about how to think about this**

The target parameter of the observed data distribution (the __estimand__) given our convenience assumptions, would be the following: 
$[\mathbb{E}(Y|A=1,W=w) - \mathbb{E}(Y|A=0,W=w)]$

## Estimate.
**Apply each of the three estimators we have learned in class (simple or non-targeted substitution estimator-a.k.a G-computation estimator), Inverse probability of treatment weighted estimator, and TMLE) to estimate your target parameter. Use of the tmle, ltmle, or other Rpackages is acceptable. Also report unadjusted results for comparison.**

```{r}
# One last step to prepare for estimation. ##
df=df%>%
  mutate(A=as.numeric(as.factor(mask_requirement))-1, populationsq=population**2, population_densitysq=population_density**2, log_population=log(population), log_popdensity=log(population_density))

df %>% 
  count(A)

# One last step to prepare for estimation. ##
df=df%>%
  mutate(A=as.numeric(as.factor(mask_requirement))-1, populationsq=population**2, population_densitysq=population_density**2, log_population=log(population), log_popdensity=log(population_density))
df %>%
  count(A)

## Taking a random subset of the dataset to make estimation easier to run. I'm keeping 5000 rows to keep it as small as possible, but we can then adjust this to whatever maximum works for implementing the code easily.
set.seed(52059)
unique_county=sqldf("SELECT DISTINCT county_name, state, A FROM df")
uc_0=unique_county %>%
  filter(A==0)
uc_1=unique_county %>%
  filter(A==1)
uc_2=unique_county %>%
  filter(A==2)
dim_uc0=dim(uc_0)[1]
dim_uc1=dim(uc_1)[1]
dim_uc2=dim(uc_2)[1]
n0=.12*dim_uc0
n1=.12*dim_uc1
n2=.12*dim_uc2
cu_0=uc_0[sample(1:nrow(uc_0), n0, replace=FALSE),]
cu_1=uc_1[sample(1:nrow(uc_1), n1, replace=FALSE),]
cu_2=uc_2[sample(1:nrow(uc_2), n2, replace=FALSE),]
only_these=as.data.frame(rbind(cu_0, cu_1, cu_2))
df_short=only_these%>%
  left_join(df)
write.csv(df_short, "df_short.csv")
```

## Be  sure  to  include  a  basic  descriptive  table  of  your  data  that  provides information on the outcome, exposure, and covariate distributions.*
```{r, echo=F, results='asis'}
# reducing just to main variables
stats <- df_short[c(9:10,12:20,23)]
stargazer(as.data.frame(stats),
          title = "Descriptive Statistics",
          digits=2,
          header = FALSE,
          covariate.labels = c("Infection Rate",
                               "Population", 
                               "Median Household Income", 
                               "Population Density", 
                               "Percent African American", 
                               "Percent Latino", "Percent Asian American", 
                               "Percent White", 
                               "Percent Native American", 
                               "Percent Pacific Islander", 
                               "Percent College", 
                               "Unemployment Rate"))
```

## G-computation estimator

$\mathbb{E}_{U,X} = \sum_{w} \mathbb{E}_0[\mathbb{E}(Y|A=a,W=w)P_0(W=w)$ 

```{r}
# G-Comp with SuperLearner #
# Commenting out for now because this is having problems, and I haven't been able to figure out what the problem is here. #

df_short <- read.csv(here("df_short.csv")) ## adding this just so I don't have to run all the code each time R breaks
set.seed(52059)
n_short = 4452 ## so that if we change this, it's easy to change here

Fold=c(rep(1, n_short), rep(2, n_short), rep(3, n_short), rep(4, n_short), rep(5, n_short), rep(6, n_short), rep(7, n_short), rep(8, n_short), rep(9, n_short), rep(10, n_short))
ObsData=data.frame(df_short, Fold)
head(ObsData)

CV.risk_L2=matrix(NA, nrow=10, ncol=4)
CV.risk_MSE=matrix(NA, nrow=10, ncol=4)

# Discrete Super Learner for loop - each estimator on training set #
for (V in 1:10) {
  ## creating validation and training set
  valid=ObsData[Fold==V,]
  train=ObsData[Fold!=V,]
  nrow(valid)
 
  ## using glm to fit each algorithm on the training set
  EstA=glm(infection_rate~population+population_density+white+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+A, data=train, family='gaussian')
 
  EstB=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+A, data=train, family='gaussian')
  
  EstC=glm(infection_rate~population+population_density+white+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(A), data=train, family='gaussian')
 
  EstD=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(A), data=train, family='gaussian')
  
  ## For each algorithm, predicting the probability of Y for each observation in the validation set
  PredA=predict(EstA, newdata=valid, type='response')
  PredB=predict(EstB, newdata=valid, type='response')
  PredC=predict(EstC, newdata=valid, type='response')
  PredD=predict(EstD, newdata=valid, type='response')
  
  ## estimating the cross-validated risk estimate for each algorithm based on the L2 loss function.
  CV.risk_L2[V,]=c(mean((valid$infection_rate-PredA)^2), mean((valid$infection_rate-PredB)^2), mean((valid$infection_rate-PredC)^2), mean((valid$infection_rate-PredD)^2))

  ## estimating the cross-validated risk estimate for each algorithm based on the MSE loss function
  CV.risk_MSE[V,]=c(MSE(PredA, valid$infection_rate), MSE(PredB, valid$infection_rate), MSE(PredC, valid$infection_rate), MSE(PredD, valid$infection_rate))
}

colMeans(CV.risk_L2)
colMeans(CV.risk_MSE)
```

**Comment BS: We're getting numbers, but I don't know how to interpret them** 

```{r}
function (Y, X, newX, family, direction="both", trace=0, k=2) {
  fit.glm=glm(infection_rate ~ ., data=ObsData, family='gaussian')
  fit.step=step(fit.glm, direction=direction, trace=trace,k=k)
  pred=predict(fit.step, newdata=newX, type="response")
  fit=list(object=fit.step)
  out=list(pred=pred, fit=fit)
  class(out$fit) =c("SL.step")
  return(out)
}

# Load wrapper
source(here("final_project_wrappers.R"))
## source('C:/Users/jessi/Desktop/ph242_final_project/final_project_wr#appers_gcomp.R')

# Library for algorithms 
# SL.library=c('SL.glm.EstA', 'SL.glm.EstB', 'SL.glm.EstC', 'SL.glm.EstD', 'SL.ridge','SL.rpartPrune', 'SL.polymars', 'SL.mean')

SL.library=c('SL.ridge')

X=subset(ObsData, select=-infection_rate)

SL.out=SuperLearner(Y=ObsData$infection_rate, X=X, SL.library=SL.library, family='gaussian',cvControl=list(V=10))

## I'm only trying one library. I tried it once and got the error "missing data is currently not supported". I ran it another time and my computer broke down. Oh well. 
SL.out
```
 
```{r}
# G-Computation (No SuperLearner) - not required for this section, but ideally if we can get the above working, that would be better!

## !!! NOTE: JUST STARTING WITH FOUR BECAUSE THIS GETS OUT OF HAND FAST - SEE final_project_wrappers.R script for "full" list (actually not full, if we were going to fully compare the clustering on state versus clustering on state and county, this would require A-H compared to I-P). ## 

# Create a dataset where the value of A is set to zero. #
Alevel_0=ObsData
Alevel_0$A=0

# Create a dataset where the value of A is set to one. #
Alevel_1=ObsData
Alevel_1$A=1

# Create a dataset where the value of A is set to two. #
Alevel_2=ObsData
Alevel_2$A=2

# Estimator 1 - White included model - MSM #
estimator_1=glm(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+A, data=ObsData)

## Commenting out clusters ## 
#library(estimatr) estimator_1=lm_robust(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+A, data=ObsData, cluster=state) #

# Note: We won't actually use all three of these later (because this is the MSM) but I'm including it here to make sure it's clear that this is the case of a three-level/ordinal treatment. #
qn1_2=predict(estimator_1, Alevel_2, type="response")
qn1_1=predict(estimator_1, Alevel_1, type="response")
qn1_0=predict(estimator_1, Alevel_0, type="response")

# Estimator 2 - African_American included model - MSM #
estimator_2=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+A, data=ObsData)
# estimator_2=lm_robust(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+A, data=ObsData, cluster=state) #
qn2_2=predict(estimator_2, Alevel_2, type="response")
qn2_1=predict(estimator_2, Alevel_1, type="response")
qn2_0=predict(estimator_2, Alevel_0, type="response")

# Estimator 3 - White Inlcuded - MSM - Population/Pop Density Sq #
estimator_3=glm(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+populationsq+population_densitysq+A, data=ObsData)
#estimator_3=lm_robust(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+populationsq+population_densitysq+A, data=ObsData, cluster=state)#

qn3_2=predict(estimator_3, Alevel_2, type="response")
qn3_1=predict(estimator_3, Alevel_1, type="response")
qn3_0=predict(estimator_3, Alevel_0, type="response")

# Estimator 4 - African_American Inlcuded - MSM - Population/Pop Density Sq #
estimator_4=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+populationsq+population_densitysq+A, data=ObsData)
#estimator_4=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+populationsq+population_densitysq+A, data=ObsData, cluster=state)

qn4_2=predict(estimator_4, Alevel_2, type="response")
qn4_1=predict(estimator_4, Alevel_1, type="response")
qn4_0=predict(estimator_4, Alevel_0, type="response")

# Estimator 5 - White Inlcuded - MSM - Population/Pop Density Log #
estimator_5=glm(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+log_population+log_popdensity+A, data=ObsData)
#estimator_5=lm_robust(infection_rate~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+log_population+log_popdensity+A, data=ObsData, cluster=state)#

qn5_2=predict(estimator_5, Alevel_2, type="response")
qn5_1=predict(estimator_5, Alevel_1, type="response")
qn5_0=predict(estimator_5, Alevel_0, type="response")

# Estimator 6 - African_American Inlcuded - MSM - Population/Pop Density Log #
estimator_6=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+log_population+log_popdensity+A, data=ObsData)
#estimator_6=glm(infection_rate~population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+as.factor(month)+log_population+log_popdensity+A, data=ObsData, cluster=state)

qn6_2=predict(estimator_6, Alevel_2, type="response")
qn6_1=predict(estimator_6, Alevel_1, type="response")
qn6_0=predict(estimator_6, Alevel_0, type="response")

# Note - the mean difference between 2 and 1 will be the same as the difference between 1 and 0 for the MSM! Therefore, I only use 1-0 here - this will have to change for the saturated models - see in later estimators. # 
psi.hat1=mean(qn1_1)-mean(qn1_0)
psi.hat2=mean(qn2_1)-mean(qn2_0)
psi.hat3=mean(qn3_1)-mean(qn3_0)
psi.hat4=mean(qn4_1)-mean(qn4_0)
psi.hat5=mean(qn5_1)-mean(qn5_0)
psi.hat6=mean(qn6_1)-mean(qn6_0)
```

```{r}
estimates=as.data.frame(cbind(psi.hat1, psi.hat2, psi.hat3, psi.hat4, psi.hat5, psi.hat6))
estimates
```

```{r}
## Come back and add the simulation to get bias, variance, etc. ##
Mean_PsiHat1=mean(estimates[,1])
Mean_PsiHat2=mean(estimates[,2])
Mean_PsiHat3=mean(estimates[,3])
Mean_PsiHat4=mean(estimates[,4])

Mean_PsiHat1; Mean_PsiHat2; Mean_PsiHat3; Mean_PsiHat4
```

## Estimate the bias of each estimator. ##
```{r}
# Bias_PsiHat1=Mean_PsiHat1-Psi.P0
# Bias_PsiHat2=Mean_PsiHat2-Psi.P0
# Bias_PsiHat3=Mean_PsiHat3-Psi.P0
# Bias_PsiHat4=Mean_PsiHat4-Psi.P0
# 
# Bias_PsiHat1; Bias_PsiHat2; Bias_PsiHat3; Bias_PsiHat4

## Comment BS: where do we get Psi.PO from?
```

## Estimate the variance of each estimator. ## 
```{r}
# forvar=as.data.frame(estimates)
# head(forvar)
# 
# Var_PsiHat1=mean((forvar$V1-Mean_PsiHat1)^2)
# Var_PsiHat2=mean((forvar$V2-Mean_PsiHat2)^2)
# Var_PsiHat3=mean((forvar$V3-Mean_PsiHat3)^2)
# Var_PsiHat4=mean((forvar$V4-Mean_PsiHat4)^2)
# 
# Var_PsiHat1; Var_PsiHat2; Var_PsiHat3; Var_PsiHat4
```

## Estimate the mean squared error or each estimator. ##
```{r}
# MSE_PsiHat1=Bias_PsiHat1^2 + Var_PsiHat1
# MSE_PsiHat2=Bias_PsiHat2^2 + Var_PsiHat2
# MSE_PsiHat3=Bias_PsiHat3^2 + Var_PsiHat3
# MSE_PsiHat4=Bias_PsiHat4^2 + Var_PsiHat4

#MSE_PsiHat1; MSE_PsiHat2; MSE_PsiHat3; MSE_PsiHat4
```

## Inverse probability of treatment weighted estimator

```{r}
# Estimator T1  - White Included#
iptw_treatreg1=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate, data=ObsData, cluster=state)#

treat_pred1=predict(iptw_treatreg1, type="probs")
gAW1=matrix(NA, nrow=n_short, ncol=3)

gAW1[ObsData$A==0] = treat_pred1[ObsData$A==0, "0"]
gAW1[ObsData$A==1] = treat_pred1[ObsData$A==1, "1"]
gAW1[ObsData$A==2] = treat_pred1[ObsData$A==2, "2"]
summary(gAW1)

# Estimator T2  - African_American Included#
iptw_treatreg2=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate, data=ObsData, cluster=state)#

treat_pred2=predict(iptw_treatreg2, type="probs")
gAW2=matrix(NA, nrow=n_short, ncol=3)

gAW2[ObsData$A==0] = treat_pred2[ObsData$A==0, "0"]
gAW2[ObsData$A==1] = treat_pred2[ObsData$A==1, "1"]
gAW2[ObsData$A==2] = treat_pred2[ObsData$A==2, "2"]
summary(gAW2)

# Estimator T3  - White Included - Pop/Pop Density sq#
iptw_treatreg3=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+populationsq+population_densitysq, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+populationsq+population_densitysq, data=ObsData, cluster=state)#

treat_pred3=predict(iptw_treatreg3, type="probs")
gAW3=matrix(NA, nrow=n_short, ncol=3)

gAW3[ObsData$A==0] = treat_pred3[ObsData$A==0, "0"]
gAW3[ObsData$A==1] = treat_pred3[ObsData$A==1, "1"]
gAW3[ObsData$A==2] = treat_pred3[ObsData$A==2, "2"]
summary(gAW3)

# Estimator T4  - African_American Included - pop/pop density sq#
iptw_treatreg4=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+populationsq+population_densitysq, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+populationsq+population_densitysq, data=ObsData, cluster=state)#

treat_pred4=predict(iptw_treatreg4, type="probs")
gAW4=matrix(NA, nrow=n_short, ncol=3)

gAW4[ObsData$A==0] = treat_pred4[ObsData$A==0, "0"]
gAW4[ObsData$A==1] = treat_pred4[ObsData$A==1, "1"]
gAW4[ObsData$A==2] = treat_pred4[ObsData$A==2, "2"]
summary(gAW4)

# Estimator T3  - White Included - Pop/Pop Density log#
iptw_treatreg5=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+log_population+log_popdensity, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+log_population+log_popdensity, data=ObsData, cluster=state)#

treat_pred5=predict(iptw_treatreg5, type="probs")
gAW5=matrix(NA, nrow=n_short, ncol=3)

gAW5[ObsData$A==0] = treat_pred5[ObsData$A==0, "0"]
gAW5[ObsData$A==1] = treat_pred5[ObsData$A==1, "1"]
gAW5[ObsData$A==2] = treat_pred5[ObsData$A==2, "2"]
summary(gAW5)

# Estimator T6  - African_American Included - pop/pop density log#
iptw_treatreg6=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+log_population+log_popdensity, data=ObsData)
# iptw_treatreg=multinom(A ~ population+population_density+african_american+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate+log_population+log_popdensity, data=ObsData, cluster=state)#

treat_pred6=predict(iptw_treatreg6, type="probs")
gAW6=matrix(NA, nrow=n_short, ncol=3)

gAW6[ObsData$A==0] = treat_pred6[ObsData$A==0, "0"]
gAW6[ObsData$A==1] = treat_pred6[ObsData$A==1, "1"]
gAW6[ObsData$A==2] = treat_pred6[ObsData$A==2, "2"]
summary(gAW6)
```

Ranges from zero (ish) to one, so that's good! Seems skewed towards higher values a bit though, too. 

```{r}
wt1=1/gAW1
wt2=1/gAW2
wt3=1/gAW3
wt4=1/gAW4
wt5=1/gAW5
wt6=1/gAW6

summary(wt1)
summary(wt2)
summary(wt3)
summary(wt4)
summary(wt5)
summary(wt6)
```

Wow! Some extremely large weights here for the max - 259,229.70! Mostly reasonable though. 

```{r}
IPTW1=mean(wt1*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt1*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT1=(mean(wt1*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt1*as.numeric(ObsData$A==2)))-(mean(wt1*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt1*as.numeric(ObsData$A==0)))

IPTW2=mean(wt2*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt2*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT2=(mean(wt2*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt2*as.numeric(ObsData$A==2)))-(mean(wt2*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt2*as.numeric(ObsData$A==0)))

IPTW3=mean(wt3*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt3*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT3=(mean(wt3*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt3*as.numeric(ObsData$A==2)))-(mean(wt3*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt3*as.numeric(ObsData$A==0)))

IPTW4=mean(wt4*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt4*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT4=(mean(wt4*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt4*as.numeric(ObsData$A==2)))-(mean(wt4*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt4*as.numeric(ObsData$A==0)))

IPTW5=mean(wt5*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt5*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT5=(mean(wt5*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt5*as.numeric(ObsData$A==2)))-(mean(wt5*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt5*as.numeric(ObsData$A==0)))

IPTW6=mean(wt6*as.numeric(ObsData$A==2)*ObsData$infection_rate)-mean(wt6*as.numeric(ObsData$A==0)*ObsData$infection_rate)

HT6=(mean(wt6*as.numeric(ObsData$A==2)*ObsData$infection_rate)/mean(wt6*as.numeric(ObsData$A==2)))-(mean(wt6*as.numeric(ObsData$A==0)*ObsData$infection_rate)/mean(wt6*as.numeric(ObsData$A==0)))

comp=as.data.frame(cbind(c("Est1","Est2", "Est3", "Est4","Est5", "Est6"), rbind(cbind(IPTW1, HT1), cbind(IPTW2, HT2), cbind(IPTW3, HT3), cbind(IPTW4, HT4), cbind(IPTW5, HT5), cbind(IPTW6, HT6))))

comp=comp%>%
  rename(
    IPTW = IPTW1,
    HT = HT1,
    Estimator = V1
    )

comp
```
HT is \textit{much} better than IPTW. These IPTW estimates are wild :) 

```{r}
## IPTW stabilized 1 ##
gA1=matrix(NA, nrow=37270)

gA1[ObsData$A==0]=mean(ObsData$A==0)
gA1[ObsData$A==1]=mean(ObsData$A==1)
gA1[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM1=gA1/gAW1

st.MSM1=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM1)
#st.MSM1=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #
summary(st.MSM1)

## IPTW stabilized 2 ##
gA2=matrix(NA, nrow=37270)

gA2[ObsData$A==0]=mean(ObsData$A==0)
gA2[ObsData$A==2]=mean(ObsData$A==2)
gA2[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM2=gA2/gAW2

st.MSM2=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM2)
#st.MSM2=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #

summary(st.MSM2)

## IPTW stabilized 3 ##
gA3=matrix(NA, nrow=37270)

gA3[ObsData$A==0]=mean(ObsData$A==0)
gA3[ObsData$A==3]=mean(ObsData$A==3)
gA3[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM3=gA3/gAW3

st.MSM3=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM3)
#st.MSM3=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #

summary(st.MSM3)

## IPTW stabilized 4 ##
gA4=matrix(NA, nrow=37270)

gA4[ObsData$A==0]=mean(ObsData$A==0)
gA4[ObsData$A==4]=mean(ObsData$A==4)
gA4[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM4=gA4/gAW4

st.MSM4=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM4)
#st.MSM4=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #

summary(st.MSM4)

## IPTW stabilized 5 ##
gA5=matrix(NA, nrow=37270)

gA5[ObsData$A==0]=mean(ObsData$A==0)
gA5[ObsData$A==5]=mean(ObsData$A==5)
gA5[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM5=gA5/gAW5

st.MSM5=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM5)
#st.MSM5=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #

summary(st.MSM5)

## IPTW stabilized 6 ##
gA6=matrix(NA, nrow=37270)

gA6[ObsData$A==0]=mean(ObsData$A==0)
gA6[ObsData$A==6]=mean(ObsData$A==6)
gA6[ObsData$A==2]=mean(ObsData$A==2)

wt.MSM6=gA6/gAW6

st.MSM6=glm(infection_rate ~ A, data=ObsData, weights=wt.MSM6)
#st.MSM6=lm_robust(infection_rate ~ A, data=ObsData, weights=wt.MSM, cluster=state) #

summary(st.MSM6)

iptw_sw_matrix=matrix(NA, nrow=6, ncol=1)
iptw_sw_matrix[1,]=st.MSM1$coefficients[2]
iptw_sw_matrix[2,]=st.MSM2$coefficients[2]
iptw_sw_matrix[3,]=st.MSM3$coefficients[2]
iptw_sw_matrix[4,]=st.MSM4$coefficients[2]
iptw_sw_matrix[5,]=st.MSM5$coefficients[2]
iptw_sw_matrix[6,]=st.MSM6$coefficients[2]

iptw_sw_matrix
```

## TMLE 
**Use Super Learner when implementing TMLE. For comparison, you may wish to use it when implementing your G-computation and IPTW estimators also. A simple library is fine (writing wrappers to include your own parametric regressions as candidates is great). Include an assessment of the performance (cross validated risk) of the algorithms in your library. It is helpful to include the simple mean as a benchmark.Also report an estimate of the cross-validated risk of the SL and interpret.**

```{r}
# SuperLearner # 
listWrappers()
SL.library=c("SL.glm", "SL.step", "SL.mean")

# Create our X dataframe, which has exposure and covariates, as well as X2 with A=2, X1 with A=1, and X0 with A=0 # 
training=ObsData %>%
  mutate(rand=runif(50000)) %>%
  filter(rand < .2) %>%
  dplyr::select(A,infection_rate,population,population_density,white,african_american,asian_american,hispanic,median_hh_income,pct_unemployed,pct_college_associate,log_population,log_popdensity,populationsq,population_densitysq)

X_train=training %>%
dplyr::select(-infection_rate, )

# Create X's for each level of A to use later! #
X0t=X_train %>%
mutate(A=0)

X1t=X_train %>%
mutate(A=1)

X2t=X_train %>%
mutate(A=2)

X=subset(df_short, select=-infection_rate)

# Estimate $\Bar{Q}_0(A,W)$ by running SuperLearner. #
QbarSL = SuperLearner(Y=df_short$infection_rate, X=X, SL.library=SL.library, family="gaussian")

# Get initial estimates of the expected outcome given exposure and covariates. # 
QbarAW=predict(QbarSL, newdata=ObsData)$pred

# Obtain initial estimates of the expected outcome for all units under different levels of exposure of A. # Qbarq2W=predict(QbarSL, newdata=X2)$pred
Qbarq1W=predict(QbarSL, newdata=X1)$pred
Qbarq0W=predict(QbarSL, newdata=X0)$pred

# Estimate the simple sub estimator (plug-in!) Again, we'll just need to do this for one level since we're working with an MSM (assuming linear impact of exposure on outcome.) #
Phi_SS_Phat=mean(Qbarq1W)-mean(Qbarq0W)
Phi_SS_Phat
```

```{r}
# Now, we need to estimate the exposure mechanism (conditional probability of exposure level, a, given baseline covariates). #
gHatSL=SuperLearner(Y=training$A, X=subset(X_train, select=-c(A)), SL.library=SL.library, family="gaussian")

#### !!!! NOTE !!!! The code we have from lab will need to be adapted here - I can't get this all to run, so I'm going to put in what I think will potentially work, assuming it'll be a few rows, with 2 and then 1? ####
gHat2W=gHatSL$SL.predict[1] ### [1] is a guess! 
gHat1W=gHatSL$SL.predict[2]
gHat0W=1-gHat2W+gHat1W # This is fine so long as we can populate the above two correctly! #

## Propensity score summary ##
summary(data.frame(gHat2W, gHat1W, gHat0W))

## Predicted probability of A given baseline ##
gHatAW=rep(NA, 37270)
gHatAW[training$A==2]=gHat2W[training$A==2]
gHatAW[training$A==1]=gHat1W[training$A==1]
gHatAW[training$A==0]=gHat0W[training$A==0]

# Check that the predicted probability of the obs exposure equal the predicted probabiity when A=a # 
tail(data.frame(training$A, gHatAW, gHat1W, gHat0W))

# Clever covariate H(A,W) for each obs #
H.AW = as.numeric(training$A==1)/gHat1W - as.numeric(training$A==0)/gHat0W 
##!!!!! Not sure if this works perfectly either - do we need to adapt this from lab given our MSM?!!! ###

# Evaluate our clever covariates # 
H.2W=1/gHat2W
H.1W=1/gHat1W
H.0W=1/gHat0W

tail(data.frame(training$A, H.AW, H.2W, H.1W, H.0W))
```

```{r}
# Aaand I think we can delete the above. WOOF. #
# install.packages("ltmle")
ltmle.SL=ltmle(data=df_short, Anodes='A', Ynodes='infection_rate', abar=list(2,1,0), SL.library=SL.library) #put data as training because it might take forever
summary(ltmle.SL)

## Comment BS: Says abar must be of length two. Maybe something else to ask?
```

```{r}
# Exploring model performance under misspecification. #
ltmle.SL=ltmle(data=training, Anodes='A', Ynodes='infection_rate', abar=list(2,1,0), Qform=c(infection_rate="Q.kplus1 ~ A+population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associat+populationsq+population_densitysq"), gform="A~population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associat+populationsq+population_densitysq", SL.library=SL.library) #put data as training because it might take forever
```
```{r}
# Explore double robustness using misspecified regression #
ltmle.DRb=ltmle(data=training, Anodes='A', Ynodes='infection_rate', abar=list(1,0), SL.library=SL.library, gform="A~U")

summary(ltmle.DRb)
```

Reporting Unadjusted Results:
```{r}
## below is similar to how we did in lab but it doesn't run...
training <- data.frame(U=1, training)
ltmle.unadj <- ltmle(data=training, Anodes='A', Ynodes='infection_rate', abar=list(1,0), 
                     SL.library=SL.library, gform="A~U")
summary(ltmle.unadj)

# computing unadjusted E(Y|A)
ObsData %>%
  group_by(mask_requirement) %>%
  summarise(infection = mean(infection_rate))
```
Comparing the expected outcomes of $Y$ (infection rates) by statewide mask requirement without adjusting for any baseline covariates, we find that average infection rates are highest in counties without any mask mandates (885.7) and lowest in those with statewide mask mandates (483.4).

**Provide some formally assessment of the positivity assumption. Evaluate the distribution of your estimated propensity score gn(A= 1|Wi), i=1, ..., n, and corresponding non-stabilized weights (as well as of your stabilized weights if you use stabilized weights to fit an MSM). Consider evaluating sensitivity to different truncation levels for gn. Note that for TMLE, bounding or truncating gn away from 0 is recommended on the basis of both theory and finite sample performance; for IPTW it can help or hurt. Report how for how many observations was gn(A|Wi) truncated.**

We show above how some of the weights are extremely high for some states because of violations to the positivity assumption. To provide more context for this, we create a series of plots showing why the estimated propensity score is so extreme in some cases.

First, we plot values of two covariates--population density and the share of Black residents--by treatment levels. While there is pretty good coverage between a statewide mandate and mandate in some parts, none of the counties that never passed a mask mandate have a percent of Black residents that exceeds 7 percent. In addition, the most densely populated counties are all in states with statewide mask mandates.
```{r}
ggplot(data = ObsData, aes(x = population_density, y = african_american, color = as.factor(mask_requirement))) +
  geom_point() + 
  scale_x_log10() +
  facet_grid( ~ mask_requirement) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(y="Percent Black/African American", x = "Population Density (logged)") +
  ggtitle("Mask Mandates (Treatment) and Baseline Covariates")
```

Similarly, when we plot total population and median household income, we see several strata with zero observations and therefore risk overfitting a non-parametric maximum likelihood estimator.
```{r}
ggplot(data = ObsData, aes(x = population, y = median_hh_income, color = as.factor(mask_requirement))) +
  geom_point() + 
  scale_x_log10() +
  facet_grid( ~ mask_requirement) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(y="Median Household Income", x = "Population (logged)") +
  ggtitle("Mask Mandates (Treatment) and Baseline Covariates")
```

```{r}
## Estimating the treatment mechanism g(A|W) = P(A|W)
gAW.reg=multinom(A ~ population+population_density+white+asian_american+hispanic+median_hh_income+pct_unemployed+pct_college_associate, data=df_shortbinary)
summary(gAW.reg)

## Predicted probabilities by treatment level
treat_pred=predict(gAW.reg, type="probs")
summary(treat_pred)
## We're bound to have very extreme weights in the IPTW given that we have minimums of zero and maximums of 1. 
gAW=matrix(NA, nrow=n_short, ncol=2)
gAW[df_shortbinary$A==0] = treat_pred[df_shortbinary$A==0, "0"]
gAW[df_shortbinary$A==1] = treat_pred[df_shortbinary$A==1, "1"]
summary(gAW)

## Each observation gets an inverse weight inverse to their predicted probability
wt <- 1/gAW

# look at the distribution of weights
summary(wt)

## Point estimate: weighted empirical mean outcome for states with mask enforcement mandate minus the weighted empirical mean for states without mask enforcement mandate
IPTW <- mean(wt * as.numeric(df_shortbinary$A==1) * df_shortbinary$infection_rate) - mean(wt*(df_shortbinary$A==0)*(df_shortbinary$infection_rate))
IPTW ## -29.02%

## The IPTW estimate of Î¨(P0) is -29.022%. We can interpret this statistical estimand as the marginal difference in the infection rate associated with mask enforcement, after controlling for measured confounders. If the identifiability assumptions (i.e. randomization and positivity) held, we could then interpret Ïˆ0 in terms of the average treatment effect (a.k.a. the causal risk difference).

## Truncate weights ARBITRARILY at 10 and 50 
sum(wt>5) ## 612 observations
wt.trunc_5 <- wt
wt.trunc_5[wt.trunc_10>5] = 5

sum(wt>50) ## 144 observations
wt.trunc_50 <- wt
wt.trunc_50[wt.trunc_50>50] = 50

sum(wt>500) ## 36 observations
wt.trunc_500 <- wt
wt.trunc_500[wt.trunc_500>500] = 500

## Evaluating the IPTW estimand with the truncated weights

## Truncated at 10
mean(wt.trunc_5*as.numeric(df_shortbinary$A==1)*df_shortbinary$infection_rate) - mean(wt.trunc_5*as.numeric(df_shortbinary$A==0)*df_sdf_shortbinaryhort$infection_rate)
## 417.17%

## Truncated at 50 
mean(wt.trunc_50*as.numeric(df_shortbinary$A==1)*df_shortbinary$infection_rate) - mean( wt.trunc_50*as.numeric(df_shortbinary$A==0)*df_shortbinary$infection_rate)
## 164.24%

## Truncated at 500
mean(wt.trunc_500*as.numeric(df_shortbinary$A==2)*df_shortbinary$infection_rate) - mean( wt.trunc_500*as.numeric(df_shortbinary$A==0)*df_shortbinary$infection_rate)
## -29.02%

## Truncation at different levels yields very different results. The more observations we eliminate, the lower estimator we get. For instance, if we truncate at weight 5, which means we lose 612 observations, we get an estimator of 417.17%, whereas if we truncate at 500, which means we lose 36 observations, we get an estimator of -29.02%, same as the IPTW without truncation. 

## Stabilized IPTW estimator
mean(wt*as.numeric(df_shortbinary$A==1)*df_shortbinary$infection_rate)/mean(wt*as.numeric(df_shortbinary$A==1)) - mean(wt*as.numeric(df_shortbinary$A==0)*df_shortbinary$infection_rate)/mean(wt*as.numeric(df_shortbinary$A==0))

## For this sample of counties, the modified Horvitz-Thompson estimator yielded an estimate of 624.89%, closest to truncation at a weight of 500. 
```

**Present a detailed plan for statistical inference/variance estimation based on the non-parametric bootstrap, and implement it (understanding that time  may be a limitation  depending on  your  SL  library). When bootstrapping an estimator that uses cross-validation to estimate nuisance parameters, be careful to ensure that all copies of a given independent unit are contained within the same fold. Plot your bootstrap distribution and comment as appropriate. For TMLE (and IPTW), you can also report a influence curve based variance estimate for comparison, if you wish.**

## 4 The non-parametric bootstrap for variance estimation

```{r}
## filtering data for non missing values 
df_short_no_na <- df_short %>% 
  filter(!is.na(infection_rate), !is.na(population), !is.na(population_density), !is.na(african_american), !is.na(asian_american), !is.na(hispanic), !is.na(median_hh_income), !is.na(pct_college_associate), !is.na(pct_unemployed), !is.na(log_population), !is.na(log_popdensity), !is.na(A), !is.na(log_popdensity)) %>%
  dplyr::select(infection_rate, population, population_density, african_american, asian_american, hispanic, median_hh_income, pct_college_associate, pct_unemployed, log_popdensity, log_population, A)

# SL.library=c("SL.glm", "SL.step", "SL.glm.interaction")
SL.library=c("SL.glm") ## specifying a smaller library to try this out

## transforming outcome to binary so that we can get the non-parametric bootstrap to run 
summary(df_short_no_na$infection_rate)
## we'll be cutting the variable in half at the median, 163.28

df_short_no_na$infection_rate_bin <- cut(df_short_no_na$infection_rate, br=c(162.28, 698.9, 7590.7),labels = c("1", "2"))

df_short_no_na <- df_short_no_na %>% filter(!is.na(infection_rate_bin))

df_short_no_na <- df_short_no_na %>% mutate(
  infection_rate_bin = case_when(
    infection_rate_bin == "1" ~ 0, 
    infection_rate_bin == "2" ~ 1,
    TRUE ~ NA_real_,
))

# number of bootstrap samples, starting with only 5, changing later to 500
B=5

# data frame for estimates based on the boot strap sample
estimates <- data.frame(matrix(NA, nrow=B, ncol=3))

# number of obs
n = 2203

# making an id variable
df_short_no_na$id <- 1:n

# Function to handcode TMLE with Super Learner
run.tmle <- function(df_short_no_na, SL.library, id=NULL){
  
  # Estimate the conditional mean outcome Qbar(A,W)
  # dataframe X with baseline covariates and exposure
  X <- subset(df_short_no_na, select=c(A, population, population_density, african_american, asian_american, hispanic,median_hh_income, pct_unemployed, pct_college_associate, log_population, log_popdensity))
  
  # set the A=2 in X2 and the A=0 in X0
  X2 <- X0 <-X
  X2$A <- 2 # under exposure
  X0$A <- 0 # under control
  
  # call Super Learner for estimation of QbarAW
  QbarSL <- SuperLearner(Y=df_short_no_na$infection_rate_bin, X=X, SL.library=SL.library, family="binomial", id=id)
  QbarSL
  
  # initial estimates of the outcome, given the observed exposure & covariates
  QbarAW <- predict(QbarSL, newdata=df_short_no_na)$pred
  # estimates of the outcome, given A=2 and covariates
  Qbar2W <- predict(QbarSL, newdata=X2)$pred
  # estimates of the outcome, given A=0 and covariates
  Qbar0W <- predict(QbarSL, newdata=X0)$pred
  
  # simple substitution estimator:
  PsiHat.SS <- mean(Qbar2W - Qbar0W)
  
  # Estimate the exposure mechanism g(A|W)
  # call Super Learner for the exposure mechanism
  gHatSL <- SuperLearner(Y=df_short_no_na$A, X=X, SL.library=SL.library, family="gaussian", id=id)
  # generate predicted prob being exposed, given baseline covariates
  gHat2W <- gHatSL$SL.predict
  # predicted prob of not being exposed, given baseline covariates
  gHat0W <- 1- gHat2W
  
  # # predicted prob of observed exposure, given baseline cov
  gHatAW <- rep(NA, n)
  gHatAW[df_short_no_na$A==2]<- gHat2W[df_short_no_na$A==2]
  gHatAW[df_short_no_na$A==0]<- gHat0W[df_short_no_na$A==0]

  #-------------------------------------------------
  # Clever covariate H(A,W) for each subject
  #-------------------------------------------------
  H.AW <- as.numeric(df_short_no_na$A==2)/gHat2W - as.numeric(df_short_no_na$A==0)/gHat0W

  # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
  H.2W <- 1/gHat2W
  H.0W <- -1/gHat0W
  
  #IPTW estimator of the G-computation formula:
  PsiHat.IPTW <- mean(H.AW*df_short_no_na$infection_rate)
  
  #------------------------------------------
  # Update the initial estimator of Qbar_0(A,W)
  #------------------------------------------
  logitUpdate<- glm(df_short_no_na$infection_rate_bin ~ -1 + offset(qlogis(QbarAW)) + H.AW, family='binomial')
  epsilon <- logitUpdate$coef
  QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
  Qbar2W.star <- plogis(qlogis(Qbar2W)+ epsilon*H.2W)
  Qbar0W.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
  #------------------------------------------
  # Estimate Psi(P_0)
  #------------------------------------------
  PsiHat.TMLE <- mean(Qbar2W.star - Qbar0W.star)
  
  #------------------------------------------
  # Return point estimates, targeted estimates of Qbar_0(A,W), and the vector of clever covariates
  #------------------------------------------
  estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.TMLE))
  Qbar.star <- data.frame(cbind(QbarAW.star, Qbar2W.star, Qbar0W.star))
  names(Qbar.star)<- c('QbarAW.star', 'Qbar2W.star', 'Qbar0W.star')
  list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
}

estimates

## The TMLE estimate of Î¨(P0) is 12%. After controlling for baseline covariates, the marginal difference in the probability of high infection rate based on some mask enforcement versus no mask enforcement is negative. 

# for loop from b=1 to total number of bootstrap samples
for(b in 1:B){
  
  # sample the indices 1 to n with replacement
  bootIndices <- sample(1:n, replace=F) ## do we need to put without replacement if we want them to be independent across folds? 
  bootData <- df_short_no_na[bootIndices,]
  
  # calling the above function
  estimates[b,]<- run.tmle(df_short_no_na=bootData, SL.library=SL.library, id=bootData$id)$estimates
  
  # keep track of the iterations completed
  print(b)
}

#---------------------------------
# Explore the bootstrapped point estimates
#---------------------------------
par(mfrow=c(3,1))
hist(estimates[,1], main="Histogram of point estimates from \nthe Simple Substitution estimator \nover B bootstrapped samples", xlab="Point Estimates")
hist(estimates[,2], main="Histogram of point estimates \nfrom IPTW estimator over \nB bootstrapped samples", xlab="Point Estimates")
hist(estimates[,3], main="Histogram of point estimates from \nTMLE over B bootstrapped samples", xlab="Point Estimates")
```

## Interpret results.
**What is the statistical interpretation of your analyses? Discuss differences (or lack thereof) in the estimates provided by the different estimators. What is the causal interpretation of your results and how plausible is it?  What are key limitations of your analysis? How might these results (if at all) inform policy, understanding
, and/or the design of future studies?**

As with any inquiry, we have some limitations with our current analysis. First, our treatment variable is not as specific as we would like to be able to isolate the impact of mask enforcement at the county level. Because enforcement is at the state-level and classified as either "enforcement in all counties of the state", "enforcement in some counties of the state" and "enforcement in no counties of the state", we do not know specifically which counties or geographic areas of the state had mask enforcement and which did not. We started out implementing clustered standard errors at the state-level - since our treatment variable pertains to a state, and each of the counties are nested within the state. However, we ran into issues running the code with SuperLearner, a big sample size and clustered standard errors, and at the advice of the instructor, we're treating data as n.i.i.d. 

Also, after discussion with instructor, and given that out treatment indicator has three levels, we started analyzing this dataset under the assumption of a MSM, in which we are assuming a linear impact of treatment on the outcome (i.e. the difference between full mask enforcement and some mask enforcement is the same as the difference between some mask enforcement and no mask enforcement. This is likely to be an unsafe assumption, given that the impact of mask enforcement is likely not linear.
